{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the pre-trained FaceNet model towards the Caltech Face Dataset (450 images, 30 peaple) using triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/miniconda3/envs/ML/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import Input\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FaceNet_utils import one_hot, distance_based_prediction, evaluate_model, face_dist, load_FaceData, load_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFace = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = K.sum(K.maximum(basic_loss,0.0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNModel = load_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeature = FNModel.output.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNModel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = Input((3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = FNModel(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/miniconda3/envs/ML/lib/python3.6/site-packages/keras/applications/resnet50.py:281: UserWarning: You are using the TensorFlow backend, yet you are using the Theano image data format convention (`image_data_format=\"channels_first\"`). For best performance, set `image_data_format=\"channels_last\"` in your Keras config at ~/.keras/keras.json.\n",
      "  warnings.warn('You are using the TensorFlow backend, yet you '\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape = (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2048), Dimension(1), Dimension(1)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling2D, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalMaxPooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "dense_1 = Dense(embedding_dim)(x)\n",
    "normalized = Lambda(lambda  x: K.l2_normalize(x,axis=1))(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model2 = Model(base_model.input, normalized, name=\"base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input((3, 224,224))\n",
    "r1 = base_model2(input_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9ca9fcb3ce45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceData, labels, labels_OH = load_FaceData()\n",
    "print(FaceData.shape)\n",
    "print(labels.shape)\n",
    "print(labels_OH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[FaceData[:10],FaceData[10:20],FaceData[20:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [np.zeros((10, 1)),np.zeros((10, 1)),np.zeros((10, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNModel.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceEmbedding = FNmodel.predict(FaceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = StandardScaler().fit_transform(FaceEmbedding)\n",
    "x_feature_pca = pca.fit_transform(x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_feature_pca[:,0],x_feature_pca[:,1],c=labels)\n",
    "plt.title('feature space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceEmbedding_Corr = np.corrcoef(FaceEmbedding)\n",
    "plt.imshow(FaceEmbedding_Corr,cmap='jet')\n",
    "plt.title('Correlation matrix')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distances_within_class = []\n",
    "Distances_among_class = []\n",
    "\n",
    "FaceEmbedding_Dist = np.ones((nFace,nFace))*100\n",
    "for i in range(nFace):\n",
    "    for j in range(i,nFace):\n",
    "        FaceEmbedding_Dist[i,j]=face_dist(FaceEmbedding[i,:],FaceEmbedding[j,:])\n",
    "        FaceEmbedding_Dist[j,i]=FaceEmbedding_Dist[i,j]\n",
    "        if labels[i]==labels[j]:\n",
    "            Distances_within_class.append(FaceEmbedding_Dist[i,j])\n",
    "        else:\n",
    "            Distances_among_class.append(FaceEmbedding_Dist[i,j])\n",
    "            \n",
    "Distances_within_class = np.array(Distances_within_class)\n",
    "Distances_among_class = np.array(Distances_among_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(FaceEmbedding_Dist,cmap='jet')\n",
    "plt.title('Distance matrix')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(Distances_among_class,bins = 50, label = 'cross class',normed = True)\n",
    "_ = plt.hist(Distances_within_class,bins = 50, label = 'same class', normed = True)\n",
    "plt.title('Distribution of cross-class distance')\n",
    "plt.legend()\n",
    "\n",
    "print('Cross-class distance = %1.3f +\\- %1.3f' % (Distances_among_class.mean(), Distances_among_class.std()))\n",
    "print('Same-class distance = %1.3f +\\- %1.3f' % (Distances_within_class.mean(), Distances_within_class.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Even before doing anything, the direct embedding of faces using FaceNet is already able to recognize faces at decent level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = []\n",
    "FN = []\n",
    "\n",
    "for threshold in np.arange(0,max(Distances_among_class),0.01):\n",
    "    FP.append(100.0*np.sum(Distances_among_class<threshold)/len(Distances_among_class))\n",
    "    FN.append(100.0*np.sum(Distances_within_class>threshold)/len(Distances_within_class))\n",
    "    \n",
    "plt.plot(FP,FN)  \n",
    "threshold = 0.68\n",
    "print('pct of false positive %2.3f %%' % (100.0*np.sum(Distances_among_class<threshold)/len(Distances_among_class)))\n",
    "print('pct of false negative %2.3f %%' % (100.0*np.sum(Distances_within_class>threshold)/len(Distances_within_class)))\n",
    "\n",
    "print('accuracy = %2.3f' % (100.0*(np.sum(Distances_among_class>threshold)+np.sum(Distances_within_class<threshold))/(len(Distances_among_class)+len(Distances_within_class))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking\n",
    "Now build a one dense layer FC NN to further process the face embedding to seperate them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPeople = len(list(set(labels)))\n",
    "X_input = Input((nFeature,))\n",
    "X = Dense(128,activation = 'relu')(X_input)\n",
    "X = Dropout(0.5, seed = 1)(X)\n",
    "X = Dense(nPeople,activation = 'softmax')(X)\n",
    "myModel = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n",
    "myModel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = []\n",
    "test_idx = []\n",
    "for l in set(labels):\n",
    "    ind = np.where(labels==l)[0]\n",
    "    if len(ind)<=1:\n",
    "        ind_train = list(ind)\n",
    "        ind_test = []\n",
    "    else:\n",
    "        i_split = max([3,int(len(ind)*0.7)])\n",
    "        ind_train = list(ind[0:i_split])\n",
    "        ind_test = list(ind[i_split:])\n",
    "    train_idx += ind_train\n",
    "    test_idx += ind_test  \n",
    "print(\"Training data set has %d face images\", len(train_idx))\n",
    "print(\"Test data set has %d face images\", len(test_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCorrect = 0\n",
    "for i in test_idx:\n",
    "    p = distance_based_prediction(FaceEmbedding[train_idx], labels[train_idx], FaceEmbedding[i])\n",
    "    if p == labels[i]:\n",
    "        nCorrect += 1\n",
    "print('Face recognition accuracy using purely distance based method: %3.3f %%' % (100.*(0.+nCorrect)/len(test_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(myModel, FaceEmbedding[train_idx], labels_OH[train_idx], FaceEmbedding[test_idx], labels_OH[test_idx], maxItem = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.fit(FaceEmbedding[train_idx], labels_OH[train_idx], epochs = 300, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(myModel, FaceEmbedding[train_idx], labels_OH[train_idx], FaceEmbedding[test_idx], labels_OH[test_idx], maxItem = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in test_idx:\n",
    "    p=np.argmax(myModel.predict(FaceEmbedding[j:j+1,:]))\n",
    "    if p!=labels[j]:\n",
    "        plt.figure()\n",
    "        plt.imshow(FaceData[j].transpose((1,2,0)))\n",
    "        plt.title('pred %d, true %d' % (p,labels[j]))\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        tmp = np.where(labels==p)[0][0]\n",
    "        plt.imshow(FaceData[tmp].transpose((1,2,0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceEmbedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
